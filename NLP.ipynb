{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2gv2i9fNDvrg"
   },
   "source": [
    "# Subreddit Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "ZG9BpbQt3-ko",
    "outputId": "f4f3306d-928a-4ee1-c74c-565198ec1862"
   },
   "outputs": [],
   "source": [
    "subreddit_train = \"coursework_subreddit_train.json\"\n",
    "subreddit_test = \"coursework_subreddit_test.json\"\n",
    "\n",
    "!gsutil cp gs://textasdata/coursework/coursework_subreddit_train.json $subreddit_train \n",
    "!gsutil cp gs://textasdata/coursework/coursework_subreddit_test.json  $subreddit_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation and analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SB5QU5cn9xFN",
    "outputId": "dbf23158-5366-4f4b-9b7f-fec03667869d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# A temporary variable to store the list of post content.\n",
    "posts_tmp = list()\n",
    "\n",
    "with open(subreddit_train) as jsonfile:\n",
    "  for i, line in enumerate(jsonfile):\n",
    "    thread = json.loads(line)\n",
    "    for post in thread['posts']:\n",
    "      # Keep the thread title and subreddit with each post.\n",
    "      posts_tmp.append((thread['title'], post.get(\"author\"), post.get('body')))\n",
    "print(len(posts_tmp))#\n",
    "# Create the posts data frame.  \n",
    "labels = ['title', 'author', 'body']\n",
    "post_frame = pd.DataFrame(posts_tmp, columns=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BRSM70t-WUEk"
   },
   "outputs": [],
   "source": [
    "# import gzip\n",
    "\n",
    "a=[]\n",
    "b=[]\n",
    "c=[]\n",
    "for i,title,author,body in post_frame.itertuples(index=True, name='Pandas'):\n",
    "  if title in a:\n",
    "    if author!=None and body!=None:\n",
    "      b[a.index(title)].append(author)\n",
    "      c[a.index(title)].append(body)\n",
    "    if author==None and body!=None:\n",
    "      b[a.index(title)].append(\"None\")\n",
    "      c[a.index(title)].append(body)\n",
    "    if author!=None and body==None:\n",
    "      b[a.index(title)].append(author)\n",
    "      c[a.index(title)].append(\"None\")\n",
    "    if author==None and body==None:\n",
    "      b[a.index(title)].append(\"None\")\n",
    "      c[a.index(title)].append(\"None\")\n",
    "    \n",
    "  else:    \n",
    "    \n",
    "    \n",
    "    if author==None and body!=None:\n",
    "      a.append(title)\n",
    "      c.append([body])\n",
    "      b.append([\"None\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    if author!=None and body!=None:\n",
    "      a.append(title)\n",
    "      c.append([body])\n",
    "      b.append([author])\n",
    "\n",
    "    if author!=None and body==None:\n",
    "      a.append(title)\n",
    "      c.append([\"None\"])\n",
    "      b.append([author])\n",
    "      \n",
    "    \n",
    "author=[]\n",
    "body=[]\n",
    "for i in b:\n",
    "  author.append(\" \".join(i))\n",
    "for i in c:\n",
    "  body.append(\" \".join(i))  \n",
    "df=pd.DataFrame()\n",
    "df[\"title\"]=a\n",
    "df[\"author\"]=author\n",
    "df[\"body\"]=body\n",
    "\n",
    "post_frame=df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rO6cj4XDIGpS",
    "outputId": "9ce8346f-5f86-4306-b941-f462b88e75b5"
   },
   "outputs": [],
   "source": [
    "posts_test = list()\n",
    "\n",
    "with open(subreddit_test) as jsonfile:\n",
    "  for i, line in enumerate(jsonfile):\n",
    "    thread = json.loads(line)\n",
    "    for post in thread['posts']:\n",
    "      # Keep the thread title and subreddit with each post.\n",
    "      posts_test.append((thread['title'], post.get(\"author\"), post.get('body')))\n",
    "print(len(posts_test))#\n",
    "# Create the posts data frame.  \n",
    "labels = ['title', 'author', 'body']\n",
    "post_frametest = pd.DataFrame(posts_test, columns=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBTT2DeRR8_w"
   },
   "outputs": [],
   "source": [
    "post_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YXmN_Q5R3a_g"
   },
   "outputs": [],
   "source": [
    "a=[]\n",
    "b=[]\n",
    "c=[]\n",
    "for i,title,author,body in post_frametest.itertuples(index=True, name='Pandas'):\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  if title in a:\n",
    "    if author!=None and body!=None:\n",
    "      b[a.index(title)].append(author)\n",
    "      c[a.index(title)].append(body)\n",
    "      \n",
    "    if author==None and body==None:\n",
    "      b[a.index(title)].append(\"None\")\n",
    "      c[a.index(title)].append(\"None\")\n",
    "    \n",
    "      \n",
    "    if author==None and body!=None:\n",
    "      b[a.index(title)].append(\"None\")\n",
    "      c[a.index(title)].append(body)\n",
    "    if author!=None and body==None:\n",
    "      \n",
    "      b[a.index(title)].append(author)\n",
    "      c[a.index(title)].append(\"None\")\n",
    "\n",
    "  else:    \n",
    "    \n",
    "    \n",
    "    if author==None and body!=None:\n",
    "      a.append(title)\n",
    "      c.append([body])\n",
    "      b.append([\"None\"])\n",
    "    \n",
    "    if author!=None and body!=None:\n",
    "      a.append(title)\n",
    "      c.append([body])\n",
    "      b.append([author])\n",
    "\n",
    "    if author!=None and body==None:\n",
    "      a.append(title)\n",
    "      c.append([\"None\"])\n",
    "      b.append([author])\n",
    "      \n",
    "    \n",
    "author=[]\n",
    "body=[]\n",
    "for i in b:\n",
    "  author.append(\" \".join(i))\n",
    "for i in c:\n",
    "  body.append(\" \".join(i))  \n",
    "df1=pd.DataFrame()\n",
    "df1[\"title\"]=a\n",
    "df1[\"author\"]=author\n",
    "df1[\"body\"]=body\n",
    "\n",
    "post_frametest=df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCEG8t6PC2f7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#post_frame\n",
    "#train_data\n",
    "train_threads = pd.read_json(path_or_buf=subreddit_train, lines=True)\n",
    "print(list(train_threads.columns.values))\n",
    "print(train_threads)\n",
    "#print(train_threads.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "89UU3g27C8SZ",
    "outputId": "b4d97dfd-f9d8-40af-c685-32ccba53588b"
   },
   "outputs": [],
   "source": [
    "test_threads = pd.read_json(path_or_buf=subreddit_test, lines=True)\n",
    "print(test_threads.head())\n",
    "print(test_threads.size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classfication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "3Nl9qzazDQ_6",
    "outputId": "c479cb9b-2593-4aa8-8538-23973030eaf7"
   },
   "outputs": [],
   "source": [
    "subreddit_counts = train_threads['subreddit'].value_counts()\n",
    "print(subreddit_counts.describe())\n",
    "top_subbreddits = subreddit_counts.nlargest(20)\n",
    "top_subbreddits_list = top_subbreddits.index.tolist()\n",
    "#print(top_subbreddits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "02OXNfo9H8Kc",
    "outputId": "5a25daa5-7236-4b5e-ca3b-8f974855761c"
   },
   "outputs": [],
   "source": [
    "train_labels = train_threads['subreddit']\n",
    "test_labels = test_threads['subreddit']\n",
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "3clX2Igo4cHy",
    "outputId": "fb31c702-70ad-420f-9ea7-3b213b370a62"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the medium english model. \n",
    "# We will use this model to get embedding features for tokens later.\n",
    "!python -m spacy download en_core_web_md\n",
    "\n",
    "nlp = spacy.load('en_core_web_md', disable=['ner'])\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('parser')\n",
    "# Download a stopword list\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "poUi-ofr3IaC"
   },
   "outputs": [],
   "source": [
    "#@Tokenize\n",
    "def spacy_tokenize(string):\n",
    "  tokens = list()\n",
    "  doc = nlp(string)\n",
    "  for token in doc:\n",
    "    tokens.append(token)\n",
    "  return tokens\n",
    "\n",
    "#@Normalize\n",
    "def normalize(tokens):\n",
    "  normalized_tokens = list()\n",
    "  for token in tokens:\n",
    "    normalized = token.text.lower().strip()\n",
    "    if ((token.is_alpha or token.is_digit)):\n",
    "      normalized_tokens.append(normalized)\n",
    "  return normalized_tokens\n",
    "  return normalized_tokens\n",
    "\n",
    "#@Tokenize and normalize\n",
    "def tokenize_normalize(string):\n",
    "  return normalize(spacy_tokenize(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8zwko6D63KsX",
    "outputId": "71ac86ef-a08e-4d50-e55a-00ea6c68ba41"
   },
   "outputs": [],
   "source": [
    "tokenize_normalize(\"the app is fun. very happy.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2yyQ_ezuObo-"
   },
   "source": [
    "### Bernoulli Naive Bayes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NDgNvlx0Sekk"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "def evaluation_summary(description, predictions, true_labels):\n",
    "  print(\"Evaluation for: \" + description)\n",
    "  precision = precision_score(predictions, true_labels,average=\"macro\")\n",
    "  recall = recall_score(predictions, true_labels,average=\"macro\")\n",
    "  accuracy = accuracy_score(predictions, true_labels)\n",
    "  f1 = fbeta_score(predictions, true_labels, 1,average=\"macro\") #1 means f_1 measure\n",
    "  print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f\" % (description,accuracy,precision,recall,f1))\n",
    "  print(classification_report(predictions, true_labels, digits=3))\n",
    "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions)) # Note the order here is true, predicted, odd.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mAd36XeBUT3K"
   },
   "outputs": [],
   "source": [
    "  #print(nb_model.predict(train_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_LzE_y_bXMSO"
   },
   "source": [
    "### Logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vTOu72NDXObt"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#lr = LogisticRegression(solver='saga')\n",
    "#lr_model = lr.fit(train_features, train_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WRQxQdOXyCCz"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.    \"\"\"\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qgywCn_QxlZr"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Use FeatureUnion to combine the features from text and summary\n",
    "prediction_pipeline = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "          transformer_list=[\n",
    "            ('text', Pipeline([\n",
    "              ('selector', ItemSelector(key='body')),\n",
    "              ('one-hot', CountVectorizer(tokenizer=tokenize_normalize, binary=True)), \n",
    "              ])),\n",
    "            ('title', Pipeline([\n",
    "              ('selector', ItemSelector(key='title')),\n",
    "              ('one-hot', CountVectorizer(tokenizer=tokenize_normalize, binary=True)), \n",
    "              ])),\n",
    "             ('name', Pipeline([\n",
    "              ('selector', ItemSelector(key='author')),\n",
    "              ('one-hot', CountVectorizer(tokenizer=tokenize_normalize, binary=True)), \n",
    "              ])),\n",
    "              \n",
    "        ])\n",
    "        )\n",
    "    ])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ubj7AkggDkH"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "prediction_pipelineno = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "          transformer_list=[\n",
    "            ('text', Pipeline([\n",
    "              ('selector', ItemSelector(key='body')),\n",
    "              ('one-hot', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
    "              ])),\n",
    "            ('title', Pipeline([\n",
    "              ('selector', ItemSelector(key='title')),\n",
    "              ('one-hot', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
    "              ])),\n",
    "             ('name', Pipeline([\n",
    "              ('selector', ItemSelector(key='author')),\n",
    "              ('one-hot', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
    "              ])),\n",
    "              \n",
    "        ])\n",
    "        )\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EIqV-8DWyKiw"
   },
   "outputs": [],
   "source": [
    "one_hot_train_features = prediction_pipeline.fit_transform(post_frame)\n",
    "#one_hot_validation_features = prediction_pipeline.transform(validation_data)\n",
    "one_hot_test_features = prediction_pipeline.transform(post_frametest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qoWLPqNZmBZZ"
   },
   "outputs": [],
   "source": [
    "one_hot_train_featuresno = prediction_pipelineno.fit_transform(post_frame)\n",
    "#one_hot_validation_features = prediction_pipeline.transform(validation_data)\n",
    "one_hot_test_featuresno = prediction_pipelineno.transform(post_frametest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1074
    },
    "colab_type": "code",
    "id": "BLawjL7kyZXM",
    "outputId": "7651f586-5829-46b2-ee03-43f4e3d5ad6e"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='saga')\n",
    "combined_model = lr.fit(one_hot_train_features,train_labels)\n",
    "evaluation_summary(\"LR TFIDF\", lr.predict(one_hot_test_features), test_labels)     ###best classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pifr2DsVpTNF"
   },
   "source": [
    "Confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "colab_type": "code",
    "id": "4pKgO2pypFb_",
    "outputId": "1642f938-ac88-4d23-808f-6081c7b8fcdb"
   },
   "outputs": [],
   "source": [
    "# Create Confusion Matrix\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "plot_confusion_matrix(test_labels, lr.predict(one_hot_test_features), test_labels, title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "EiLoLOnl2u93",
    "outputId": "41543154-add8-4341-8984-32397dbc6e24"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='saga')\n",
    "combined_model = lr.fit(one_hot_train_features,train_labels)\n",
    "#evaluation_summary(\"LR TFIDF\", lr.predict(one_hot_train_features), train_labels)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WDBD5IfCmvyD"
   },
   "source": [
    "Starting TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "3ozOel_zmInW",
    "outputId": "29f86532-fde9-4966-8243-f4b5d9bf9066"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='saga')\n",
    "combined_model = lr.fit(one_hot_train_featuresno,train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1096
    },
    "colab_type": "code",
    "id": "8liN9iqwmi1l",
    "outputId": "99c3eac5-93be-43ff-85fc-11a93dd172f7"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "combined_model = lr.fit(one_hot_train_featuresno,train_labels)\n",
    "evaluation_summary(\"LR TFIDF\", lr.predict(one_hot_test_featuresno), test_labels)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Clu2kew4w6b1"
   },
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "PNIOZrvrw87K",
    "outputId": "f9e86c56-3550-4a3c-d410-13257e84c5ef"
   },
   "outputs": [],
   "source": [
    " from sklearn import svm\n",
    "X = [[0, 0], [1, 1]]\n",
    " y = [0, 1]\n",
    " clf = svm.SVC(gamma='scale',kernel='rbf')\n",
    " clf.fit(one_hot_train_features,train_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1027
    },
    "colab_type": "code",
    "id": "5AXC1KUh4_ul",
    "outputId": "e851c1dc-2e97-4fe1-85bc-c63425bd05cc"
   },
   "outputs": [],
   "source": [
    "evaluation_summary(\"SVM HOT\", clf.predict(one_hot_test_features), test_labels)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1027
    },
    "colab_type": "code",
    "id": "7c0InmoG5E_c",
    "outputId": "ab9ab197-097c-4499-83cc-24e32e597a1c"
   },
   "outputs": [],
   "source": [
    "evaluation_summary(\"SVM tf\", clf.predict(one_hot_test_featuresno), test_labels)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1947
    },
    "colab_type": "code",
    "id": "11PjHj-sKlRC",
    "outputId": "47b135b3-8e3e-4335-d3af-f451c1a646ab"
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_prior = DummyClassifier(strategy='stratified')\n",
    "dummy_prior.fit(one_hot_train_features, train_labels)\n",
    "print(dummy_prior.score(one_hot_test_features, test_labels))\n",
    "evaluation_summary(\"Dummy Prior\", dummy_prior.predict(one_hot_test_features), test_labels)\n",
    "\n",
    "dummy_mf = DummyClassifier(strategy='most_frequent')\n",
    "dummy_mf.fit(one_hot_train_features, train_labels)\n",
    "print(dummy_mf.score(one_hot_test_features, test_labels))\n",
    "evaluation_summary(\"Dummy Majority\", dummy_mf.predict(one_hot_test_features), test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1947
    },
    "colab_type": "code",
    "id": "k80bjVZ3Qnp9",
    "outputId": "512ad795-4354-484a-91b3-502bb0dbb55a"
   },
   "outputs": [],
   "source": [
    "dummy_prior1 = DummyClassifier(strategy='stratified')\n",
    "dummy_prior1.fit(one_hot_train_featuresno, train_labels)\n",
    "print(dummy_prior1.score(one_hot_test_featuresno, test_labels))\n",
    "evaluation_summary(\"Dummy Prior\", dummy_prior1.predict(one_hot_test_featuresno), test_labels)\n",
    "\n",
    "dummy_mf1 = DummyClassifier(strategy='most_frequent')\n",
    "dummy_mf1.fit(one_hot_train_featuresno, train_labels)\n",
    "print(dummy_mf1.score(one_hot_test_featuresno, test_labels))\n",
    "evaluation_summary(\"Dummy Majority\", dummy_mf1.predict(one_hot_test_featuresno), test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Csb2H0ONWLOU"
   },
   "source": [
    "###Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "1c73xP7aXGoJ",
    "outputId": "7f2c0b66-6981-42d0-9f5f-e87cc3b130f2"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(one_hot_train_features, train_labels)\n",
    "#evaluation_summary(\"rf hot\", rf.predict(one_hot_train_features), train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1062
    },
    "colab_type": "code",
    "id": "ge2694fXWNiO",
    "outputId": "6ec9e4f8-f8bf-45bc-b074-a41be500c757"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(one_hot_train_features, train_labels)\n",
    "evaluation_summary(\"rf hot\", rf.predict(one_hot_test_features), test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1409
    },
    "colab_type": "code",
    "id": "lau_vlpPX-1k",
    "outputId": "24b80ab2-044e-4719-b235-d1625e1578a7"
   },
   "outputs": [],
   "source": [
    "rf1 = RandomForestClassifier()\n",
    "\n",
    "rf1.fit(one_hot_train_featuresno, train_labels)\n",
    "evaluation_summary(\"rf tf\", rf.predict(one_hot_train_featuresno), train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1027
    },
    "colab_type": "code",
    "id": "2g5MHIxMXkn1",
    "outputId": "6fb72f4e-78db-46c3-d3b2-7ef6b040bc65"
   },
   "outputs": [],
   "source": [
    "\n",
    "evaluation_summary(\"rf tf\", rf.predict(one_hot_test_featuresno), test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R2ZHefXzi_-T"
   },
   "source": [
    "bar chart \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "2ekAj51vqmOu",
    "outputId": "32dcb375-8678-4aab-e558-113492a03c12"
   },
   "outputs": [],
   "source": [
    "\n",
    "sorted((pd.DataFrame(test_labels).subreddit.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1B7bGjSIfFue"
   },
   "source": [
    "  queation twoooooooooo\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bfth5vP0NCbf"
   },
   "source": [
    "# Categorical classification with texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hLGtQ3kBr7_c"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "# prediction_pipeline = Pipeline([\n",
    "#               ('selector', ItemSelector(key='body')),\n",
    "#               ('one-hot', CountVectorizer(tokenizer=tokenize_normalize)),\n",
    "#               ('logreg', LogisticRegression(solver='saga'))\n",
    "#               ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prediction_pipelineno = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "          transformer_list=[\n",
    "            ('text', Pipeline([\n",
    "              ('selector', ItemSelector(key='body')),\n",
    "              ('one-hot', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf=False,max_features=None,ngram_range=(1,1))), \n",
    "              ])),\n",
    "            ('title', Pipeline([\n",
    "              ('selector', ItemSelector(key='title')),\n",
    "              ('one-hot', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf=True,max_features=None,ngram_range=(1,1))), \n",
    "              ])),\n",
    "             ('name', Pipeline([\n",
    "              ('selector', ItemSelector(key='author')),\n",
    "              ('one-hot', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf=False,max_features=None,ngram_range=(1,1))), \n",
    "              ])),\n",
    "              \n",
    "        ])\n",
    "\n",
    "        ),              \n",
    "        ('logreg', LogisticRegression(solver='saga',C= 10, multi_class = \"multinomial\")),\n",
    "\n",
    "    \n",
    "    ])\n",
    "                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1062
    },
    "colab_type": "code",
    "id": "AXLaWZrE6nCB",
    "outputId": "2860f3db-562a-445d-e419-61d93123c08b"
   },
   "outputs": [],
   "source": [
    "\n",
    "prediction_pipelineno.fit(post_frame, train_labels)\n",
    "evaluation_summary(\"LR\", prediction_pipelineno.predict(post_frametest), test_labels) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iTQl3M8SlHT4"
   },
   "source": [
    "### Error analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jdPKdAYY8BLD"
   },
   "outputs": [],
   "source": [
    "def print_errors(labels, predictions, data):\n",
    "  label_arr=labels.values\n",
    "  for idx, prediction in enumerate(predictions):\n",
    "    label=label_arr[idx]\n",
    "    if prediction != label:\n",
    "      print(post_frametest.iloc[idx,1], label, prediction, \"                        \" ,'label:', label,\"              \", 'prediction:', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2277
    },
    "colab_type": "code",
    "id": "gFdLtwhvlWUc",
    "outputId": "446420ed-e1fb-4329-9b09-b1c9052ce329"
   },
   "outputs": [],
   "source": [
    "\n",
    "print_errors(test_labels, prediction_pipelineno.predict(post_frametest), post_frametest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CADV7KKnxKIS"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "post_frame1 = post_frame\n",
    "post_frametest1 =post_frametest\n",
    "# post_frame1[\"body_length\"]= post_frame1.apply(lambda row: str(len(row.body)), axis=1)\n",
    "# post_frametest1[\"body_length\"]= post_frametest1.apply(lambda row: str(len(row.body)), axis=1)\n",
    "\n",
    "\n",
    "# #post_frametest1\n",
    "# post_frame.drop([\"body_length\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJG4RXuqG5I0"
   },
   "source": [
    "VaderSentiment for emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "mWCwmdKbf35v",
    "outputId": "a7114cf5-3998-4e64-c98b-de38bc658e91"
   },
   "outputs": [],
   "source": [
    "# x = \"noob\"\n",
    "# if \"noob\" in post_frame.iloc[1,2]:\n",
    "#    print (\"success\")\n",
    "# post_frame.iloc[34,2]\n",
    "#range(len((post_frame)))\n",
    "\n",
    "!pip install vaderSentiment\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "sentence = \"im feeling good\"\n",
    "vs = analyzer.polarity_scores(post_frame1.iloc[2,2])\n",
    "vs=analyzer.polarity_scores(sentence)\n",
    "print(vs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nm-A-_lga90d"
   },
   "source": [
    "vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "334Y69SzCPln"
   },
   "outputs": [],
   "source": [
    "\n",
    "#!pip install vaderSentiment\n",
    "#adding emotions \n",
    "\n",
    "y = list()\n",
    "t = list(range(0,len(post_frame1)))\n",
    "d = list(range(0,len(post_frame1)))\n",
    "\n",
    "for i in range(len(post_frame1)):\n",
    "  t[i] = analyzer.polarity_scores(post_frame1.iloc[i,2])\n",
    "  if t[i]['compound'] >= 0.05:\n",
    "     d[i] = \"positive\"\n",
    "      \n",
    "  elif t[i]['compound'] <=  -0.05:\n",
    "     d[i] = \"negative\"\n",
    "      \n",
    "  else: \n",
    "    d[i] = \"neutral\"\n",
    "    #print('Success!')\n",
    "  y.append(d[i])\n",
    "\n",
    "post_frame1[\"mood\"]= y\n",
    "\n",
    "#okenize_normalize(vs)\n",
    "# Example code:\n",
    "# for i in range(len(post_frame1)):\n",
    "#     vs = analyzer.polarity_scores(post_frame1.iloc[i,2])\n",
    "#     print(\"{:-<65} {}\".format(post_frame1.iloc[i,2], str(vs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iP3ITJ5LQ-Jh"
   },
   "outputs": [],
   "source": [
    "y1 = list()\n",
    "t1 = list(range(0,len(post_frametest1)))\n",
    "d1 = list(range(0,len(post_frametest1)))\n",
    "\n",
    "for i in range(len(post_frametest1)):\n",
    "  t1[i] = analyzer.polarity_scores(post_frametest1.iloc[i,2])\n",
    "  if t1[i]['compound'] >= 0.05:\n",
    "     d1[i] = \"positive\"\n",
    "      \n",
    "  elif t1[i]['compound'] <=  -0.05:\n",
    "     d1[i] = \"negative\"\n",
    "      \n",
    "  else: \n",
    "    d1[i] = \"neutral\"\n",
    "    #print('Success!')\n",
    "  y1.append(d1[i])\n",
    "\n",
    "post_frametest1[\"mood\"]= y1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qZhD_Ch8bBrL"
   },
   "source": [
    "token search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BwZKHg2EiRHF",
    "outputId": "cc034ada-96b3-4680-fa1b-0e2588b7d9a4"
   },
   "outputs": [],
   "source": [
    "x = list()\n",
    "l= list(range(0,len(post_frame1)))\n",
    "\n",
    "for i in range(len(post_frame1)):\n",
    "  if 'riot' in post_frame1.iloc[i,2]:\n",
    "    l[i] = 1\n",
    "  elif \"Riot\" in post_frame1.iloc[i,2]:\n",
    "    l[i] = 1\n",
    "  elif \"LCS\" in post_frame1.iloc[i,2]:\n",
    "    l[i] = 1\n",
    "  elif \"lcs\" in post_frame1.iloc[i,2]:\n",
    "    l[i] = 1\n",
    "  elif \"jungler\" in post_frame1.iloc[i,2]:\n",
    "    l[i] = 1\n",
    "  elif \"laner\" in post_frame1.iloc[i,2]:\n",
    "    l[i] = 1\n",
    "  else: \n",
    "    l[i] = 0\n",
    "    #print('Success!')\n",
    "  x.append(str(l[i]))  \n",
    "  \n",
    "    \n",
    "print(len([i for i, e in enumerate(x) if e == str(1)]))\n",
    "post_frame1[\"League\"]= x\n",
    "post_frame1 =pd.DataFrame(post_frame)\n",
    "#post_frame1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJkno1lM2gv_"
   },
   "outputs": [],
   "source": [
    "x1 = list()\n",
    "l1= list(range(0,len(post_frametest1)))\n",
    "\n",
    "for i in range(len(post_frametest1)):\n",
    "  if 'riot' in post_frametest1.iloc[i,2]:\n",
    "    l1[i] = 1\n",
    "  elif \"Riot\" in post_frametest1.iloc[i,2]:\n",
    "    l1[i] = 1\n",
    "  elif \"LCS\" in post_frametest1.iloc[i,2]:\n",
    "    l1[i] = 1\n",
    "  elif \"lcs\" in post_frametest1.iloc[i,2]:\n",
    "    l1[i] = 1\n",
    "  elif \"jungler\" in post_frametest1.iloc[i,2]:\n",
    "    l1[i] = 1\n",
    "  elif \"laner\" in post_frame1.iloc[i,2]:\n",
    "    l[i] = 1\n",
    "  else:\n",
    "    l1[i] = 0\n",
    "    #print('Success!')\n",
    "  x1.append(str(l1[i]))  \n",
    "len([i for i, e in enumerate(x1) if e == str(1)])\n",
    "post_frametest1[\"League\"]= x1\n",
    "post_frametest1 = pd.DataFrame(post_frametest1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dHvDCTr01Fid"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "prediction_pipelineno1 = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "          transformer_list=[\n",
    "            ('text', Pipeline([\n",
    "              ('selector', ItemSelector(key='body')),\n",
    "              ('one-hot', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf=False,max_features=None,ngram_range=(1,1))), \n",
    "              ])),\n",
    "              \n",
    "            ('title', Pipeline([\n",
    "              ('selector', ItemSelector(key='title')),\n",
    "              ('one-hot', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf=True,max_features=None,ngram_range=(1,1))), \n",
    "              ])),\n",
    "              \n",
    "            ('name', Pipeline([\n",
    "             ('selector', ItemSelector(key='author')),\n",
    "             ('one-hot', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf=False,max_features=None,ngram_range=(1,1))), \n",
    "             ])),\n",
    "              \n",
    "           ('mood', Pipeline([\n",
    "            ('selector', ItemSelector(key='mood')),\n",
    "            ('one-hot', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
    "            ])),\n",
    "              \n",
    "           ('league', Pipeline([\n",
    "            ('selector', ItemSelector(key='League')),\n",
    "            ('one-hot', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
    "              \n",
    "            ]))\n",
    "              \n",
    "              \n",
    "        ])\n",
    "\n",
    "        ),              \n",
    "        ('logreg', LogisticRegression(solver='saga',C= 1000,multi_class = \"multinomial\")),\n",
    "\n",
    "    \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1062
    },
    "colab_type": "code",
    "id": "NjsJx0uA2NRt",
    "outputId": "9977fdef-732c-4a5e-ffb4-5cdf327c66ba"
   },
   "outputs": [],
   "source": [
    "train_labels = train_threads['subreddit']\n",
    "test_labels = test_threads['subreddit']\n",
    "prediction_pipelineno1.fit(post_frame1, train_labels)\n",
    "evaluation_summary(\"LR\", prediction_pipelineno1.predict(post_frametest1), test_labels) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-G938e8D5qn"
   },
   "source": [
    "## Discourse prediction ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "9KM6aJtSETPY",
    "outputId": "8d634a4c-709d-4e74-978d-b106b684b70d"
   },
   "outputs": [],
   "source": [
    "discourse_train = \"coursework_discourse_train.json\"\n",
    "discourse_test = \"coursework_discourse_test.json\"\n",
    "  \n",
    "!gsutil cp gs://textasdata/coursework/coursework_discourse_train.json $discourse_train  \n",
    "!gsutil cp gs://textasdata/coursework/coursework_discourse_test.json  $discourse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pwOaf_6aD-Vh"
   },
   "outputs": [],
   "source": [
    "# The reddit thread structure is nested with posts in a new content.\n",
    "# This block reads the file as json and creates a new data frame.\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def load_posts(file):\n",
    "  #########################################################################\n",
    " # return pd.DataFrame(posts_tmp, columns=labels)\n",
    "  posts_tmp = list()\n",
    "\n",
    "  with open(file) as jsonfile:\n",
    "    for i, line in enumerate(jsonfile):\n",
    "     # if (i > 2): break\n",
    "      thread = json.loads(line)\n",
    "      for post in thread['posts']:\n",
    "        # NOTE: This could be changed to use additional features from the post or thread.\n",
    "        # DO NOT change the labels for the test set.\n",
    "        posts_tmp.append((thread['subreddit'], thread['title'],str(thread['is_self_post']),#str(post.get['is_self_post', \"\"]),\n",
    "                     post.get('author', \"\"), post.get('body', \"\"), post.get('majority_type', \"\"),post.get(\"majority_link\", \"\"),\n",
    "                          post.get('in_reply_to', \"\"), (post.get(\"is_first_post\"))\n",
    "                       ))\n",
    "\n",
    "# Create the posts data frame.  \n",
    "  labels = ['subreddit', 'title',\"is_self_post\", 'author', 'body', \n",
    "          \"discourse_type\",\"majority_link\",\"in_reply_to\",\"is_first_post\"]\n",
    "  return pd.DataFrame(posts_tmp, columns=labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "PDzHTDcmEQ11",
    "outputId": "d1e503de-d252-467b-8b5f-8515a78a0b2f"
   },
   "outputs": [],
   "source": [
    "train_posts = load_posts(discourse_train)\n",
    "# Filter out empty labels\n",
    "train_posts = train_posts[train_posts['discourse_type'] != \"\"]\n",
    "print(train_posts.head())\n",
    "print(\"Num posts: \", train_posts.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L9fVMWY40Sqt"
   },
   "outputs": [],
   "source": [
    "(train_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dqGLzyTOGadY"
   },
   "source": [
    "The label for the post we will be predicting is in the discourse_type column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7vvo7hfCEmvj",
    "outputId": "e00d264f-505e-4fe0-a3a4-00bc03a7e371"
   },
   "outputs": [],
   "source": [
    "test_posts = load_posts(discourse_test)\n",
    "# Filter out empty labels\n",
    "test_posts = test_posts[test_posts['discourse_type'] != \"\"]\n",
    "print(\"Num posts: \", test_posts.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jat55HhNHGBp"
   },
   "outputs": [],
   "source": [
    "train_labels = train_posts['discourse_type']\n",
    "test_labels = test_posts['discourse_type']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFl6sM58HNFp"
   },
   "source": [
    "Examine the distribution over labels on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "n3NbLPBhFOkp",
    "outputId": "6920c7c1-5e60-47b5-8f31-402632b9427b"
   },
   "outputs": [],
   "source": [
    "discourse_counts = train_labels.value_counts()\n",
    "print(discourse_counts.describe())\n",
    "\n",
    "top_discourse = discourse_counts.nlargest(200)\n",
    "print(top_discourse)\n",
    "top_discourse = top_discourse.index.tolist()\n",
    "print(top_discourse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4q13Zi67SZE"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "labels = ['subreddit', 'title', 'url', 'author', 'body', 'majority_link', \n",
    "           'discourse_type', 'in_reply_to']\n",
    "\n",
    "prediction_pipelinetfq3 = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "          transformer_list=[\n",
    "#             ('subreddit', Pipeline([\n",
    "#               ('selector', ItemSelector(key='subreddit')),\n",
    "#               ('one-hot', TfidfVectorizer(tokenizer=tokenize_normalize,ngram_range=(0,3))), \n",
    "#               ])),\n",
    "              \n",
    "            ('title', Pipeline([\n",
    "              ('selector', ItemSelector(key='title')),\n",
    "              ('tf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf=True)), \n",
    "              ])),\n",
    "              \n",
    "              \n",
    "            ('name', Pipeline([\n",
    "             ('selector', ItemSelector(key='author')),\n",
    "             ('tf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
    "                           #   ('one-hot', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf=False,max_features=None,ngram_range=(1,1))), \n",
    "\n",
    "             ])),\n",
    "              \n",
    "\n",
    "     ##################extra         \n",
    "               \n",
    "              ('body', Pipeline([\n",
    "              ('selector', ItemSelector(key='body')),\n",
    "              ('tf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
    "              ])),\n",
    "           \n",
    "              \n",
    "\n",
    "              ('majority', Pipeline([\n",
    "              ('selector', ItemSelector(key='majority_link')),\n",
    "              ('tf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
    "              ])),\n",
    "              \n",
    "              ('reply', Pipeline([\n",
    "              ('selector', ItemSelector(key='in_reply_to')),\n",
    "              ('tf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
    "              ])),\n",
    "              \n",
    "              ('self', Pipeline([\n",
    "              ('selector', ItemSelector(key='is_self_post')),\n",
    "              ('tf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
    "              ])),\n",
    "              \n",
    "        ])\n",
    "\n",
    "        ),              \n",
    "        ('logreg', LogisticRegression(solver='saga',C= 1000,multi_class = \"multinomial\")),\n",
    "\n",
    "    \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "colab_type": "code",
    "id": "Ux5aPhFQ7qIw",
    "outputId": "5034570e-51b7-4e4c-c0f3-17d07929f811"
   },
   "outputs": [],
   "source": [
    "prediction_pipelinetfq3.fit(train_posts, train_labels)\n",
    "evaluation_summary( \"LR\", prediction_pipelinetfq3.predict(test_posts), test_labels) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "colab_type": "code",
    "id": "g4_h4xUcDdFk",
    "outputId": "54f0206c-7a98-4b4f-aa07-8e5730da480f"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_labels, prediction_pipelinetfq3.predict(test_posts), test_labels, title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382833
    },
    "colab_type": "code",
    "id": "GgSy-FTMESV9",
    "outputId": "9075370f-845d-4e71-8727-64ab03d97921"
   },
   "outputs": [],
   "source": [
    "def print_errors(labels, predictions, data):\n",
    "  label_arr=labels.values\n",
    "  for idx, prediction in enumerate(predictions):\n",
    "    label=label_arr[idx]\n",
    "    if prediction != label:\n",
    "      print(test_posts.iloc[idx,4], label, prediction, \"                        \" ,'label:', label,\"              \", 'prediction:', prediction)\n",
    "print_errors(test_labels, prediction_pipelinetfq3.predict(test_posts), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gwc-JWY2rXBX"
   },
   "outputs": [],
   "source": [
    "def spacy_tokenize(string):\n",
    "  tokens = list()\n",
    "  doc = nlp(string)\n",
    "  for token in doc:\n",
    "    tokens.append(token)\n",
    "  return tokens\n",
    "\n",
    "#@Normalize\n",
    "def normalize(tokens):\n",
    "  normalized_tokens = list()\n",
    "  for token in tokens:\n",
    "    normalized = token.text.lower().strip()\n",
    "    if ((token.is_alpha or token.is_digit)):\n",
    "      normalized_tokens.append(normalized)\n",
    "  return normalized_tokens\n",
    "  return normalized_tokens\n",
    "\n",
    "#@Tokenize and normalize\n",
    "def tokenize_normalize(string):\n",
    "  return normalize(spacy_tokenize(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bq5Pm832ICEC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class AverageEmbeddingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dimension = 300\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "      \n",
    "    def transform(self, X):  \n",
    "      # Skip OOV terms. \n",
    "      # Return 0 for all dimensions if no words are in the vocabulary.\n",
    "      dense_matrix =  np.array([ \n",
    "          np.mean([token.vector for token in self.tokenizer(doc) if not token.is_oov]\n",
    "                or [np.zeros(self.dimension)], axis=0)\n",
    "          for doc in X\n",
    "      ])\n",
    "      return dense_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y0IHdzLZu_fp"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "class baseTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "      return None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "      \n",
    "    def transform(self, X):  \n",
    "      return X[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_9HNNsz9rJvm"
   },
   "outputs": [],
   "source": [
    "train_posts[\"body_length\"]= train_posts.apply(lambda row: len((row.body)), axis=1)\n",
    "test_posts[\"body_length\"]= test_posts.apply(lambda row: len((row.body)), axis=1)\n",
    "\n",
    "\n",
    "# embedding_vectorizer = AverageEmbeddingTransformer(spacy_tokenize)\n",
    "# train_embedding_features = embedding_vectorizer.transform(train_posts['body_length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gsAPYuq7Dk25"
   },
   "source": [
    "Transform is first post in to 1 and 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qodnGOwcr-P5"
   },
   "outputs": [],
   "source": [
    "#train_posts[\"body_length\"] = train_embedding_features\n",
    "# x = list()\n",
    "# l= list(range(0,len(post_frame1)))\n",
    "\n",
    "# for i in range(len(post_frame1)):\n",
    "#   if 'riot' in post_frame1.iloc[i,2]:\n",
    "#     l[i] = 1\n",
    "#   elif \"Riot\" in post_frame1.iloc[i,2]:\n",
    "#     l[i] = 1\n",
    "#   elif \"LCS\" in post_frame1.iloc[i,2]:\n",
    "#     l[i] = 1\n",
    "#   elif \"lcs\" in post_frame1.iloc[i,2]:\n",
    "#     l[i] = 1\n",
    "#   elif \"jungler\" in post_frame1.iloc[i,2]:\n",
    "#     l[i] = 1\n",
    "#   elif \"laner\" in post_frame1.iloc[i,2]:\n",
    "#     l[i] = 1\n",
    "#   else: \n",
    "#     l[i] = 0\n",
    "#     #print('Success!')\n",
    "#   x.append(str(l[i]))  \n",
    "  \n",
    "x = list()\n",
    "l= list(range(0,len(train_posts)))\n",
    "\n",
    "\n",
    "#   for t in range(len(train_posts)):\n",
    "    \n",
    "#     if str(train_posts.iloc[t,8])==True:\n",
    "#       l[t] = 1\n",
    "#     else: \n",
    "#       l[t] = 0\n",
    "#       x.append((l[t]))\n",
    "# x\n",
    "\n",
    "for i in range(len(train_posts)):\n",
    "    if 'True' in str(train_posts.iloc[i,8]):\n",
    "      l[i] = str(1)\n",
    "    else:\n",
    "      l[i]=str(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_posts[\"is_first_post\"] =l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2pgqkwnQZZ1y"
   },
   "outputs": [],
   "source": [
    "x = list()\n",
    "l= list(range(0,len(test_posts)))\n",
    "\n",
    "for i in range(len(test_posts)):\n",
    "    if 'True' in str(test_posts.iloc[i,8]):\n",
    "      l[i] = str(1)\n",
    "    else:\n",
    "      l[i]=str(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_posts[\"is_first_post\"] =l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iSmonqOQCs-b"
   },
   "source": [
    "Adding follow up question - answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ib5N-lscCzw_"
   },
   "outputs": [],
   "source": [
    "x12 = list()\n",
    "l12= [\"others\"]*len(train_posts)\n",
    "\n",
    "\n",
    "#   for t in range(len(train_posts)):\n",
    "    \n",
    "#     if str(train_posts.iloc[t,8])==True:\n",
    "#       l[t] = 1\n",
    "#     else: \n",
    "#       l[t] = 0\n",
    "#       x.append((l[t]))\n",
    "# x\n",
    "\n",
    "for i in range(len(train_posts)):\n",
    "    if \"1\" in str(train_posts.iloc[i,8]):\n",
    "      l12[i] = \"potential_question\"\n",
    "      l12[i+1]=\"potential_answer\"\n",
    "      l12[i+2]=\"others\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#l12\n",
    "train_posts[\"follow_posts\"] =l12\n",
    "#train_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jhM0lOc9-HFi"
   },
   "outputs": [],
   "source": [
    "x2 = list()\n",
    "l2= [\"others\"]*len(test_posts)\n",
    "\n",
    "\n",
    "#   for t in range(len(train_posts)):\n",
    "    \n",
    "#     if str(train_posts.iloc[t,8])==True:\n",
    "#       l[t] = 1\n",
    "#     else: \n",
    "#       l[t] = 0\n",
    "#       x.append((l[t]))\n",
    "# x\n",
    "\n",
    "for i in range(len(test_posts)):\n",
    "    if \"1\" in str(test_posts.iloc[i,8]):\n",
    "      l2[i] = \"potential_question\"\n",
    "      l2[i+1]=\"potential_answer\"\n",
    "      l2[i+2]=\"others\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#test_posts\n",
    "\n",
    "test_posts[\"follow_posts\"] =l2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZYiZmIkaHNE3"
   },
   "source": [
    "VaderSentiment for double emtion outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NLDKAGjkHhMY",
    "outputId": "171ba23b-b73e-444c-c03d-79821e1a6af2"
   },
   "outputs": [],
   "source": [
    "!pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "y = list()\n",
    "t = list(range(0,len(train_posts)))\n",
    "d = list(range(0,len(train_posts)))\n",
    "\n",
    "for i in range(len(train_posts)):\n",
    "  t[i] = analyzer.polarity_scores(train_posts.iloc[i,4])\n",
    "  if t[i]['compound'] >= 0.05:\n",
    "     d[i] = \"positive\"\n",
    "      \n",
    "  elif t[i]['compound'] <=  -0.05:\n",
    "     d[i] = \"negative\"\n",
    "      \n",
    "  else: \n",
    "    d[i] = \"neutral\"\n",
    "    #print('Success!')\n",
    "  y.append(d[i])\n",
    "\n",
    "  \n",
    "train_posts[\"mood\"]= y\n",
    "# post_frame1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90qBX9-Mtb_n"
   },
   "outputs": [],
   "source": [
    "y = list()\n",
    "t = list(range(0,len(test_posts)))\n",
    "d = list(range(0,len(test_posts)))\n",
    "\n",
    "for i in range(len(test_posts)):\n",
    "  t[i] = analyzer.polarity_scores(test_posts.iloc[i,4])\n",
    "  if t[i]['compound'] >= 0.05:\n",
    "     d[i] = \"positive\"\n",
    "      \n",
    "  elif t[i]['compound'] <=  -0.05:\n",
    "     d[i] = \"negative\"\n",
    "      \n",
    "  else: \n",
    "    d[i] = \"neutral\"\n",
    "    #print('Success!')\n",
    "  y.append(d[i])\n",
    "\n",
    "  \n",
    "test_posts[\"mood\"]= y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "orsuRnemPQlH"
   },
   "source": [
    "Link posts or self wrote "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jCmjMCMpIfTr"
   },
   "outputs": [],
   "source": [
    "x = list()\n",
    "linktrain= list(range(0,len(train_posts)))\n",
    "\n",
    "for i in range(len(train_posts)):\n",
    "    if 'www' in (train_posts.iloc[i,4]):\n",
    "      linktrain[i] = \"linkpost\"\n",
    "      \n",
    "    elif \"http:\" in (train_posts.iloc[i,4]):\n",
    "      linktrain[i]= \"linkpost\"\n",
    "      \n",
    "    else:\n",
    "      linktrain[i]=\"selfpost\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_posts[\"is_link_post\"] =linktrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJDBpbmXRrfH"
   },
   "outputs": [],
   "source": [
    "\n",
    "linktest= list(range(0,len(test_posts)))\n",
    "\n",
    "for i in range(len(test_posts)):\n",
    "    if 'www' in (test_posts.iloc[i,4]):\n",
    "      linktest[i] = \"linkpost\"\n",
    "      \n",
    "    elif \"http:\" in (test_posts.iloc[i,4]):\n",
    "      linktest[i]= \"linkpost\"\n",
    "      \n",
    "    else:\n",
    "      linktest[i]=\"selfpost\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_posts[\"is_link_post\"] =linktest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Zb_wBF5TojQ"
   },
   "source": [
    "Symbols "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7wO9wWY-Tpip"
   },
   "outputs": [],
   "source": [
    "\n",
    "symbols= list(range(0,len(test_posts)))\n",
    "\n",
    "\n",
    "for i in range(len(test_posts)):\n",
    "  if '?' in test_posts.iloc[i,4]:\n",
    "    symbols[i] = \"question\"\n",
    "  elif \"!\" in test_posts.iloc[i,4]:\n",
    "    symbols[i] = \"superise\"\n",
    "  elif \":)\" in test_posts.iloc[i,4]:\n",
    "    symbols[i] = \"happy\"\n",
    "  else:\n",
    "    symbols[i] = \"others\"\n",
    "#test_posts[\"League\"]= x1\n",
    "#test_posts = pd.DataFrame(test_posts)\n",
    "test_posts[\"symbols_analysis\"]= symbols\n",
    "\n",
    "\n",
    "symbols= list(range(0,len(train_posts)))\n",
    "\n",
    "for i in range(len(train_posts)):\n",
    "  if '?' in train_posts.iloc[i,4]:\n",
    "    symbols[i] = \"question\"\n",
    "  elif \"!\" in train_posts.iloc[i,4]:\n",
    "    symbols[i] = \"superise\"\n",
    "  elif \":)\" in train_posts.iloc[i,4]:\n",
    "    symbols[i] = \"happy\"\n",
    "  else:\n",
    "    symbols[i] = \"others\"\n",
    "train_posts[\"symbols_analysis\"]= symbols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ttAHZSnI2gm8"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "labels = ['subreddit', 'title', 'url', 'author', 'body', 'majority_link', \n",
    "           'discourse_type', 'in_reply_to']\n",
    "\n",
    "prediction_pipelinetfq4 = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "          transformer_list=[\n",
    "            ('subreddit', Pipeline([\n",
    "              ('selector', ItemSelector(key='subreddit')),\n",
    "              ('one-hot', TfidfVectorizer(tokenizer=tokenize_normalize,ngram_range=(0,3))), \n",
    "              ])),\n",
    "              \n",
    "            ('title', Pipeline([\n",
    "              ('selector', ItemSelector(key='title')),\n",
    "              ('tf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf=True)), \n",
    "              ])),\n",
    "              \n",
    "              \n",
    "            ('name', Pipeline([\n",
    "             ('selector', ItemSelector(key='author')),\n",
    "             ('tf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
    "                           #   ('one-hot', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf=False,max_features=None,ngram_range=(1,1))), \n",
    "\n",
    "             ])),\n",
    "              \n",
    "             ('follow', Pipeline([\n",
    "             ('selector', ItemSelector(key='follow_posts')),\n",
    "             ('tf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
    "                           #   ('one-hot', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf=False,max_features=None,ngram_range=(1,1))), \n",
    "\n",
    "             ])),\n",
    "              \n",
    "\n",
    "     ##################extra         \n",
    "               \n",
    "              ('body', Pipeline([\n",
    "              ('selector', ItemSelector(key='body')),\n",
    "              ('tf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
    "              ])),\n",
    "              \n",
    "#               ('body_length', Pipeline([\n",
    "#               ('selector', ItemSelector(key='body_length')),\n",
    "#               ('one-hot', baseTransformer()), \n",
    "#               ])),\n",
    "              \n",
    "\n",
    "              ('majority', Pipeline([\n",
    "              ('selector', ItemSelector(key='majority_link')),\n",
    "              ('tf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
    "              ])),\n",
    "              \n",
    "              ('reply', Pipeline([\n",
    "              ('selector', ItemSelector(key='in_reply_to')),\n",
    "              ('tf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
    "              ])),\n",
    "              \n",
    "              ('self', Pipeline([\n",
    "              ('selector', ItemSelector(key='is_self_post')),\n",
    "              ('tf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
    "              ])),\n",
    "              \n",
    "              ('first', Pipeline([\n",
    "              ('selector', ItemSelector(key='is_first_post')),\n",
    "              ('tf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
    "              ])),\n",
    "              \n",
    "              ('mood', Pipeline([\n",
    "              ('selector', ItemSelector(key='mood')),\n",
    "              ('tf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
    "              ])),\n",
    "              \n",
    "              ('link', Pipeline([\n",
    "              ('selector', ItemSelector(key='is_link_post')),\n",
    "              ('tf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
    "              ])),\n",
    "              \n",
    "              ('symbol', Pipeline([\n",
    "              ('selector', ItemSelector(key='symbols_analysis')),\n",
    "              ('tf', TfidfVectorizer(tokenizer=tokenize_normalize)), \n",
    "              ])),\n",
    "              \n",
    "              \n",
    "        ])\n",
    "\n",
    "        ),              \n",
    "        ('logreg', LogisticRegression(solver='saga',C= 1000,multi_class = \"multinomial\")),\n",
    "\n",
    "    \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "colab_type": "code",
    "id": "ki4bDvxHVT01",
    "outputId": "1dc964e2-9c09-4271-880e-0457ea3c01e9"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = prediction_pipelinetfq4.fit(train_posts, train_labels)\n",
    "evaluation_summary( \"LR\", prediction_pipelinetfq4.predict(test_posts), test_labels) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "pbVa1fkZGHVQ",
    "outputId": "efd16731-cb74-46db-99f6-2e8abae541d9"
   },
   "outputs": [],
   "source": [
    "# Define the features\n",
    "X_train = train_posts[\"discourse_type\"]#features from training data\n",
    "X_test = test_posts[\"discourse_type\"]#features from test data\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier(strategy='stratified',random_state=0)\n",
    "clf.fit(X_train, train_posts['discourse_type'])\n",
    "predictions = clf.predict(X_test)  \n",
    "print(classification_report(predictions, test_posts['discourse_type']))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TAD.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
